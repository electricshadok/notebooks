{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNQZPNGtrv+Hb+W6DNrAflT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/electricshadok/notebooks/blob/main/gan_toy_example.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  Understanding Generative Adversarial Network (GAN)\n",
        "\n",
        "A simple Generative Adversarial Network (GAN) that learns a 2D distribution."
      ],
      "metadata": {
        "id": "2r47p6mP8PYG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Packages"
      ],
      "metadata": {
        "id": "30nG9a71-de4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm.notebook import tqdm\n",
        "import torch\n",
        "import torch.nn as nn"
      ],
      "metadata": {
        "id": "aevX9nW4-hOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Generate Synthetic Data"
      ],
      "metadata": {
        "id": "Nth5nPHt-laP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We create a dataset of 2D points sampled from a Gaussian Mixture Model (GMM). This simulates data from a real-world distribution that we want the GAN to learn."
      ],
      "metadata": {
        "id": "D3VXK0u_-w6T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "NUM_POINTS = 1000  # Total number of points\n",
        "SEED = 42  # Seed for reproducibility\n",
        "\n",
        "# Create synthetic data: Gaussian mixture\n",
        "def sample_gaussian_mixture(n_samples, means, covs, seed=None):\n",
        "    if seed is not None:\n",
        "        np.random.seed(seed)\n",
        "    n_components = len(means)\n",
        "    samples = []\n",
        "    for _ in range(n_samples):\n",
        "        k = np.random.choice(n_components)  # Randomly select one Gaussian component\n",
        "        sample = np.random.multivariate_normal(means[k], covs[k])\n",
        "        samples.append(sample)\n",
        "    return np.array(samples)\n",
        "\n",
        "# Parameters of the Gaussian mixture\n",
        "means = [[0, 0], [3, 3], [-3, -3]]\n",
        "covs = [np.eye(2) * 0.5, np.eye(2) * 0.8, np.eye(2) * 0.3]\n",
        "\n",
        "# Generate data\n",
        "X = sample_gaussian_mixture(NUM_POINTS, means, covs, seed=SEED)\n",
        "\n",
        "# Visualize the synthetic data\n",
        "plt.scatter(X[:, 0], X[:, 1], s=5, alpha=0.7)\n",
        "plt.title(\"True Data Distribution (Gaussian Mixture)\")\n",
        "plt.xlabel(\"x\")\n",
        "plt.ylabel(\"y\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "fCpnLh1z-yKk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Generative Adversarial Network (GAN)"
      ],
      "metadata": {
        "id": "pmPKNBRo_LWf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The GAN consists of two models:\n",
        "- **Generator**: Maps random noise\n",
        "- **Discriminator**: Distinguishes between real data and generated data."
      ],
      "metadata": {
        "id": "UF_n6v4P2RsT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generator Model\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, noise_dim, output_dim):\n",
        "        super(Generator, self).__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(noise_dim, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, output_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, z):\n",
        "        return self.net(z)\n",
        "\n",
        "# Discriminator Model\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(input_dim, 128),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Linear(128, 128),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Linear(128, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)"
      ],
      "metadata": {
        "id": "WY2jQ6Od_OOC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Train the Energy Model"
      ],
      "metadata": {
        "id": "9-70M-da2nVz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Initialize the Models and Optimizers**\n",
        "\n",
        "The criterion is a **Binary Cross-Entropy Loss (BCE Loss)** and ranges in $[0, \\infty)$.\n",
        "\n",
        "\n",
        "- $\\hat{y_i}$: Predicted probability (output of the model, typically between $0$ and $1$).\n",
        "- $\\hat{y_i}$ Ground truth label  ($0$ or $1$).\n",
        "- $N$: Number of samples in the batch.\n",
        "\n",
        "$$\\text{BCE Loss} = -\\frac{1}{N} \\sum_{i=1}^N \\left[ y_i \\cdot \\log(\\hat{y}_i) + (1 - y_i) \\cdot \\log(1 - \\hat{y}_i) \\right]$$\n"
      ],
      "metadata": {
        "id": "owqo57FY4CeW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model dimensions\n",
        "noise_dim = 2  # Dimension of the noise vector\n",
        "data_dim = 2   # Dimension of the data (x, y)\n",
        "\n",
        "# Instantiate models\n",
        "generator = Generator(noise_dim, data_dim)\n",
        "discriminator = Discriminator(data_dim)\n",
        "\n",
        "# Optimizers\n",
        "lr = 0.001\n",
        "gen_optimizer = torch.optim.Adam(generator.parameters(), lr=lr)\n",
        "disc_optimizer = torch.optim.Adam(discriminator.parameters(), lr=lr)\n",
        "\n",
        "# Loss function\n",
        "bce_loss = nn.BCELoss()  # Binary cross-entropy loss\n"
      ],
      "metadata": {
        "id": "aXYPWhsg36tg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Training the GAN**\n",
        "$ $\n",
        "\n",
        "**Stage 1- Discriminator Training**\n",
        "\n",
        "1. Real Data\n",
        "    - Randomly sample a batch of real data points from the dataset.\n",
        "    - Pass them through the discriminator to compute predictions $P(\\text{real}|x)$\n",
        "\n",
        "2. Fake Data\n",
        "    - Generate a batch of fake data using the generator.\n",
        "    - Pass it through the discriminator to compute predictions $P(\\text{real}|x_{fake})$\n",
        "\n",
        "3. Use binary cross-entropy loss\n",
        "    - Real data labeled as $1$\n",
        "    - Fake data labeled as $0$\n",
        "\n",
        "4. Update Discriminator\n",
        "    - Backpropagate and optimize the discriminator’s parameters\n",
        "\n",
        "Note: Often, the discriminator is trained more frequently than the generator to stabilize learning (e.g., 2-5 updates for each generator update)\n",
        "\n",
        "**Stage 2- Generator Training**\n",
        "\n",
        "1. Generate Fake Data\n",
        "    - Use random noise as input to the generator to produce fake samples.\n",
        "\n",
        "2. Discriminator Feedback\n",
        "    - Pass the fake data through the discriminator.\n",
        "    - The discriminator predicts the probability that the fake data is real.\n",
        "\n",
        "3. Use binary cross-entropy loss\n",
        "    - The generator tries to fool the discriminator, so the fake data is labeled as $1$.\n",
        "\n",
        "4. Update Discriminator\n",
        "    - Backpropagate and optimize the generator’s parameters\n"
      ],
      "metadata": {
        "id": "Ke2fCZQb4EoY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training Loop\n",
        "NUM_EPOCHS = 1000\n",
        "BATCH_SIZE = 128\n",
        "NUM_DISC_UPDATES = 2\n",
        "\n",
        "X_tensor = torch.tensor(X, dtype=torch.float32)\n",
        "\n",
        "for epoch in tqdm(range(NUM_EPOCHS), desc=\"Training Progress\"):\n",
        "    # Stage 1 - Discriminator Training\n",
        "    for _ in range(NUM_DISC_UPDATES):\n",
        "        # sample real data\n",
        "        idx = np.random.randint(0, len(X), size=BATCH_SIZE)\n",
        "        real_data = X_tensor[idx]\n",
        "\n",
        "        # generate fake data\n",
        "        z = torch.randn(BATCH_SIZE, noise_dim)\n",
        "        fake_data = generator(z)\n",
        "\n",
        "        # discriminator predictions\n",
        "        real_pred = discriminator(real_data)\n",
        "        fake_pred = discriminator(fake_data.detach())\n",
        "\n",
        "        # discriminator loss\n",
        "        real_loss = bce_loss(real_pred, torch.ones_like(real_pred))  # Real data labeled as 1\n",
        "        fake_loss = bce_loss(fake_pred, torch.zeros_like(fake_pred))  # Fake data labeled as 0\n",
        "        disc_loss = real_loss + fake_loss\n",
        "\n",
        "        # update discriminator\n",
        "        disc_optimizer.zero_grad()\n",
        "        disc_loss.backward()\n",
        "        disc_optimizer.step()\n",
        "\n",
        "    # Stage 2 - Generator Training\n",
        "    z = torch.randn(BATCH_SIZE, noise_dim)\n",
        "    fake_data = generator(z)\n",
        "    fake_pred = discriminator(fake_data)\n",
        "\n",
        "    # generator loss\n",
        "    gen_loss = bce_loss(fake_pred, torch.ones_like(fake_pred))\n",
        "\n",
        "    # update generator\n",
        "    gen_optimizer.zero_grad()\n",
        "    gen_loss.backward()\n",
        "    gen_optimizer.step()\n",
        "\n",
        "    # Optional: Print losses every few epochs\n",
        "    if epoch % 100 == 0:\n",
        "        print(f\"Epoch [{epoch}/{NUM_EPOCHS}], Discriminator Loss: {disc_loss.item()}, Generator Loss: {gen_loss.item()}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "Tlji1Fmo67DC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Visualization"
      ],
      "metadata": {
        "id": "HpV5nYdzeUtq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_samples = 1000\n",
        "noise = torch.randn(n_samples, noise_dim)\n",
        "gen_X = generator(noise).detach().numpy()\n",
        "\n",
        "plt.scatter(gen_X[:, 0], gen_X[:, 1], s=5, alpha=0.7, label=\"Generated\")\n",
        "#plt.scatter(X[:, 0], X[:, 1], s=5, alpha=0.7, label=\"True Data\")\n",
        "plt.title(\"GAN Generated Samples vs True Data\")\n",
        "plt.xlabel(\"x\")\n",
        "plt.ylabel(\"y\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "z2IXX5gzeax4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}